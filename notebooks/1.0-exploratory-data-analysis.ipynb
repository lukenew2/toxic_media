{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10932aa5",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis \n",
    "---\n",
    "*Disclaimer: This project contains text that is profane, vulgar, and offensive due to the nature of the dataset.*\n",
    "\n",
    "We live in an age where people's lives have become intertwined with their online presence allowing humans to engage with one another on a larger scale than ever before.  However, not all of these interactions are friendly.  People exploit the fact that their identity remains hidden and choose to target one another with unwarranted abuse causing harm to people they will never meet in person.  For our society to truly prosper from the digital age we have to combat this toxic behavior.  This will enable more and more people who are scared of what other people think when they post on social media to engage freely without the fear of being the target of online hate.  \n",
    "\n",
    "Our task is to improve civility on social media platforms (e.g., Twitter) and online comment forums (e.g., Reddit) by training a model that determines how likely a users comment will make another user leave a conversation.  With our model, we will create a web application that tracks how toxic each social media platform and online comment forum is to bring awareness to this issue and spark initiative to create a toxic free environment for all users.  \n",
    "\n",
    "This issue can only be solved with the help of everyone encouraging kindness instead of spreading hate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb820bb",
   "metadata": {},
   "source": [
    "## Data Description\n",
    "The dataset used in this project is from the kaggle competition: [Jigsaw Unintended Bias in Toxicity Classification](https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification/data).\n",
    "\n",
    "In 2017 the Civil Comments platform shutdown and released their archive of ~2 million public comments for researchers to study in an effort to improve civility online. Jigsaw funded the annotation of this data by human raters. \n",
    "\n",
    "The column `toxicity` is our toxicity label which contains a number between 0-1 denoting the fraction of human labelers that believed the comment would make someone else leave a conversation. \n",
    "\n",
    "For our analysis we'll define the comment as toxic (denoted 1) when the value of our target `toxicity` is greater than or equal to 0.5 otherwise we'll assign the instance to the negative class (denoted 0).\n",
    "\n",
    "There are a lot of additional labels denoting the fraction of human labelers who believed the comment depicted several other sub-toxic labels and whether specific identity attributes were mentioned in the comment. These subtype attributes are:\n",
    "* severe_toxicity\n",
    "* obscene\n",
    "* threat\n",
    "* insult\n",
    "* identity_attack\n",
    "* sexual_explicit\n",
    "* male\n",
    "* female\n",
    "* transgender\n",
    "* other_gender\n",
    "* heterosexual\n",
    "* homosexual_gay_or_lesbian\n",
    "* bisexual\n",
    "* other_sexual_orientation\n",
    "* christian\n",
    "* jewish\n",
    "* muslim\n",
    "* hindu\n",
    "* buddhist\n",
    "* atheist\n",
    "* other_religion\n",
    "* black\n",
    "* white\n",
    "* asian\n",
    "* latino\n",
    "* other_race_or_ethnicity\n",
    "* physical_disability\n",
    "* intellectual_or_learning_disability\n",
    "* psychiatric_or_mental_illness\n",
    "* other_disability\n",
    "\n",
    "It was also mentioned that different comments might have the exact same text, but labeled with different targets or sybtype attributes.\n",
    "\n",
    "These subtype attributes will be **removed** from the training schema because we will not have access to them in a production environment.  However, they will be very resourceful as tags used during error analysis to see where exactly our model is performing poorly on.\n",
    "\n",
    "### Labeling Schema\n",
    "As mentioned on Kaggle, each comment was shown to up to 10 human labelers.  The labelers were asked to rate how toxic each comment is. \n",
    "* Very Toxic\n",
    "* Toxic\n",
    "* Hard to Say\n",
    "* Not Toxic\n",
    "\n",
    "The ratings were then aggragated into the `target` column.\n",
    "\n",
    "The subtype attributes were collected by asking the human labelers whether the attribute was mentioned in the comment.  Then, taking these responses and aggregating them into a value between 0-1 representing the fraction of human labelers who believed the subtype attribute was mentioned in the comment.\n",
    "\n",
    "*Note: Some comments were shown to more than 10 human labelers because of sampling strategies to increase rating accuracy.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e44666c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import textblob\n",
    "import wordcloud\n",
    "\n",
    "# Root directory used to navigate tree.\n",
    "PROJECT_ROOT_DIR = os.path.dirname(os.getcwd())\n",
    "\n",
    "# Path to save images and figures.\n",
    "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"reports/figures\")\n",
    "if not os.path.exists(IMAGES_PATH):\n",
    "    os.makedirs(IMAGES_PATH, exist_ok=True)\n",
    "    \n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    \"\"\"\n",
    "    Saves figures in toxic_media/reports/figures.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    fig_id : str\n",
    "        Name of the figure to be saved.\n",
    "    tight_layout : bool, default=True\n",
    "        Enables padding between figure edge and edges of subplots when set to True.\n",
    "    fig_extension : {'jpg', 'png', 'svg'}, default='png'\n",
    "        Figure format.\n",
    "    resolution : int\n",
    "        Figure resolution.\n",
    "    \"\"\"\n",
    "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
    "    print(f\"Saving figure: {fig_id}\")\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92ebaf8",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "We're going to start by loading the raw data to get a quick look at it before creating a training set with which we'll use to explore in-depth. As mentioned above, the initial data file has extra columns we will not be using.  Therefore, we'll only load in the target column and the comment text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be8a5710",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = os.path.join(PROJECT_ROOT_DIR, \"data\")\n",
    "\n",
    "def load_data(path=\"raw/all_data.csv\"):\n",
    "    \"\"\"\n",
    "    Load data into pandas dataframe object.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    path : str, default=raw/all_data.csv\n",
    "        Directory and filename in format 'directory/filename.csv'.\n",
    "    \"\"\"\n",
    "    csv_path = os.path.join(DATA_DIR, path)\n",
    "    \n",
    "    return pd.read_csv(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c50651c",
   "metadata": {},
   "outputs": [],
   "source": [
    "toxic = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20c94e0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>split</th>\n",
       "      <th>created_date</th>\n",
       "      <th>publication_id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>article_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>funny</th>\n",
       "      <th>wow</th>\n",
       "      <th>...</th>\n",
       "      <th>white</th>\n",
       "      <th>asian</th>\n",
       "      <th>latino</th>\n",
       "      <th>other_race_or_ethnicity</th>\n",
       "      <th>physical_disability</th>\n",
       "      <th>intellectual_or_learning_disability</th>\n",
       "      <th>psychiatric_or_mental_illness</th>\n",
       "      <th>other_disability</th>\n",
       "      <th>identity_annotator_count</th>\n",
       "      <th>toxicity_annotator_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1083994</td>\n",
       "      <td>He got his money... now he lies in wait till a...</td>\n",
       "      <td>train</td>\n",
       "      <td>2017-03-06 15:21:53.675241+00</td>\n",
       "      <td>21</td>\n",
       "      <td>NaN</td>\n",
       "      <td>317120</td>\n",
       "      <td>approved</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>650904</td>\n",
       "      <td>Mad dog will surely put the liberals in mental...</td>\n",
       "      <td>train</td>\n",
       "      <td>2016-12-02 16:44:21.329535+00</td>\n",
       "      <td>21</td>\n",
       "      <td>NaN</td>\n",
       "      <td>154086</td>\n",
       "      <td>approved</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5902188</td>\n",
       "      <td>And Trump continues his lifelong cowardice by ...</td>\n",
       "      <td>train</td>\n",
       "      <td>2017-09-05 19:05:32.341360+00</td>\n",
       "      <td>55</td>\n",
       "      <td>NaN</td>\n",
       "      <td>374342</td>\n",
       "      <td>approved</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7084460</td>\n",
       "      <td>\"while arresting a man for resisting arrest\".\\...</td>\n",
       "      <td>test</td>\n",
       "      <td>2016-11-01 16:53:33.561631+00</td>\n",
       "      <td>13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>149218</td>\n",
       "      <td>approved</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5410943</td>\n",
       "      <td>Tucker and Paul are both total bad ass mofo's.</td>\n",
       "      <td>train</td>\n",
       "      <td>2017-06-14 05:08:21.997315+00</td>\n",
       "      <td>21</td>\n",
       "      <td>NaN</td>\n",
       "      <td>344096</td>\n",
       "      <td>approved</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 46 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                       comment_text  split  \\\n",
       "0  1083994  He got his money... now he lies in wait till a...  train   \n",
       "1   650904  Mad dog will surely put the liberals in mental...  train   \n",
       "2  5902188  And Trump continues his lifelong cowardice by ...  train   \n",
       "3  7084460  \"while arresting a man for resisting arrest\".\\...   test   \n",
       "4  5410943     Tucker and Paul are both total bad ass mofo's.  train   \n",
       "\n",
       "                    created_date  publication_id  parent_id  article_id  \\\n",
       "0  2017-03-06 15:21:53.675241+00              21        NaN      317120   \n",
       "1  2016-12-02 16:44:21.329535+00              21        NaN      154086   \n",
       "2  2017-09-05 19:05:32.341360+00              55        NaN      374342   \n",
       "3  2016-11-01 16:53:33.561631+00              13        NaN      149218   \n",
       "4  2017-06-14 05:08:21.997315+00              21        NaN      344096   \n",
       "\n",
       "     rating  funny  wow  ...  white  asian  latino  other_race_or_ethnicity  \\\n",
       "0  approved      0    0  ...    NaN    NaN     NaN                      NaN   \n",
       "1  approved      0    0  ...    NaN    NaN     NaN                      NaN   \n",
       "2  approved      1    0  ...    NaN    NaN     NaN                      NaN   \n",
       "3  approved      0    0  ...    NaN    NaN     NaN                      NaN   \n",
       "4  approved      0    0  ...    NaN    NaN     NaN                      NaN   \n",
       "\n",
       "   physical_disability  intellectual_or_learning_disability  \\\n",
       "0                  NaN                                  NaN   \n",
       "1                  NaN                                  NaN   \n",
       "2                  NaN                                  NaN   \n",
       "3                  NaN                                  NaN   \n",
       "4                  NaN                                  NaN   \n",
       "\n",
       "   psychiatric_or_mental_illness  other_disability  identity_annotator_count  \\\n",
       "0                            NaN               NaN                         0   \n",
       "1                            NaN               NaN                         0   \n",
       "2                            NaN               NaN                         0   \n",
       "3                            NaN               NaN                         0   \n",
       "4                            NaN               NaN                         0   \n",
       "\n",
       "   toxicity_annotator_count  \n",
       "0                        67  \n",
       "1                        76  \n",
       "2                        63  \n",
       "3                        76  \n",
       "4                        80  \n",
       "\n",
       "[5 rows x 46 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toxic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d4309c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1999516 entries, 0 to 1999515\n",
      "Data columns (total 46 columns):\n",
      " #   Column                               Dtype  \n",
      "---  ------                               -----  \n",
      " 0   id                                   int64  \n",
      " 1   comment_text                         object \n",
      " 2   split                                object \n",
      " 3   created_date                         object \n",
      " 4   publication_id                       int64  \n",
      " 5   parent_id                            float64\n",
      " 6   article_id                           int64  \n",
      " 7   rating                               object \n",
      " 8   funny                                int64  \n",
      " 9   wow                                  int64  \n",
      " 10  sad                                  int64  \n",
      " 11  likes                                int64  \n",
      " 12  disagree                             int64  \n",
      " 13  toxicity                             float64\n",
      " 14  severe_toxicity                      float64\n",
      " 15  obscene                              float64\n",
      " 16  sexual_explicit                      float64\n",
      " 17  identity_attack                      float64\n",
      " 18  insult                               float64\n",
      " 19  threat                               float64\n",
      " 20  male                                 float64\n",
      " 21  female                               float64\n",
      " 22  transgender                          float64\n",
      " 23  other_gender                         float64\n",
      " 24  heterosexual                         float64\n",
      " 25  homosexual_gay_or_lesbian            float64\n",
      " 26  bisexual                             float64\n",
      " 27  other_sexual_orientation             float64\n",
      " 28  christian                            float64\n",
      " 29  jewish                               float64\n",
      " 30  muslim                               float64\n",
      " 31  hindu                                float64\n",
      " 32  buddhist                             float64\n",
      " 33  atheist                              float64\n",
      " 34  other_religion                       float64\n",
      " 35  black                                float64\n",
      " 36  white                                float64\n",
      " 37  asian                                float64\n",
      " 38  latino                               float64\n",
      " 39  other_race_or_ethnicity              float64\n",
      " 40  physical_disability                  float64\n",
      " 41  intellectual_or_learning_disability  float64\n",
      " 42  psychiatric_or_mental_illness        float64\n",
      " 43  other_disability                     float64\n",
      " 44  identity_annotator_count             int64  \n",
      " 45  toxicity_annotator_count             int64  \n",
      "dtypes: float64(32), int64(10), object(4)\n",
      "memory usage: 701.7+ MB\n"
     ]
    }
   ],
   "source": [
    "toxic.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61f0724a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                                     0.000000e+00\n",
       "comment_text                           5.001210e-07\n",
       "split                                  0.000000e+00\n",
       "created_date                           0.000000e+00\n",
       "publication_id                         0.000000e+00\n",
       "parent_id                              4.325082e-01\n",
       "article_id                             0.000000e+00\n",
       "rating                                 0.000000e+00\n",
       "funny                                  0.000000e+00\n",
       "wow                                    0.000000e+00\n",
       "sad                                    0.000000e+00\n",
       "likes                                  0.000000e+00\n",
       "disagree                               0.000000e+00\n",
       "toxicity                               0.000000e+00\n",
       "severe_toxicity                        0.000000e+00\n",
       "obscene                                0.000000e+00\n",
       "sexual_explicit                        0.000000e+00\n",
       "identity_attack                        0.000000e+00\n",
       "insult                                 0.000000e+00\n",
       "threat                                 0.000000e+00\n",
       "male                                   7.759458e-01\n",
       "female                                 7.759458e-01\n",
       "transgender                            7.759458e-01\n",
       "other_gender                           7.759458e-01\n",
       "heterosexual                           7.759458e-01\n",
       "homosexual_gay_or_lesbian              7.759458e-01\n",
       "bisexual                               7.759458e-01\n",
       "other_sexual_orientation               7.759458e-01\n",
       "christian                              7.759458e-01\n",
       "jewish                                 7.759458e-01\n",
       "muslim                                 7.759458e-01\n",
       "hindu                                  7.759458e-01\n",
       "buddhist                               7.759458e-01\n",
       "atheist                                7.759458e-01\n",
       "other_religion                         7.759458e-01\n",
       "black                                  7.759458e-01\n",
       "white                                  7.759458e-01\n",
       "asian                                  7.759458e-01\n",
       "latino                                 7.759458e-01\n",
       "other_race_or_ethnicity                7.759458e-01\n",
       "physical_disability                    7.759458e-01\n",
       "intellectual_or_learning_disability    7.759458e-01\n",
       "psychiatric_or_mental_illness          7.759458e-01\n",
       "other_disability                       7.759458e-01\n",
       "identity_annotator_count               0.000000e+00\n",
       "toxicity_annotator_count               0.000000e+00\n",
       "dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toxic.isnull().sum() / len(toxic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bda72c28",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>publication_id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>article_id</th>\n",
       "      <th>funny</th>\n",
       "      <th>wow</th>\n",
       "      <th>sad</th>\n",
       "      <th>likes</th>\n",
       "      <th>disagree</th>\n",
       "      <th>toxicity</th>\n",
       "      <th>...</th>\n",
       "      <th>white</th>\n",
       "      <th>asian</th>\n",
       "      <th>latino</th>\n",
       "      <th>other_race_or_ethnicity</th>\n",
       "      <th>physical_disability</th>\n",
       "      <th>intellectual_or_learning_disability</th>\n",
       "      <th>psychiatric_or_mental_illness</th>\n",
       "      <th>other_disability</th>\n",
       "      <th>identity_annotator_count</th>\n",
       "      <th>toxicity_annotator_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1.999516e+06</td>\n",
       "      <td>1.999516e+06</td>\n",
       "      <td>1.134709e+06</td>\n",
       "      <td>1.999516e+06</td>\n",
       "      <td>1.999516e+06</td>\n",
       "      <td>1.999516e+06</td>\n",
       "      <td>1.999516e+06</td>\n",
       "      <td>1.999516e+06</td>\n",
       "      <td>1.999516e+06</td>\n",
       "      <td>1.999516e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>448000.000000</td>\n",
       "      <td>448000.000000</td>\n",
       "      <td>448000.000000</td>\n",
       "      <td>448000.000000</td>\n",
       "      <td>448000.000000</td>\n",
       "      <td>448000.000000</td>\n",
       "      <td>448000.000000</td>\n",
       "      <td>448000.000000</td>\n",
       "      <td>1.999516e+06</td>\n",
       "      <td>1.999516e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>4.065400e+06</td>\n",
       "      <td>4.988997e+01</td>\n",
       "      <td>3.715138e+06</td>\n",
       "      <td>2.810257e+05</td>\n",
       "      <td>2.776687e-01</td>\n",
       "      <td>4.437174e-02</td>\n",
       "      <td>1.089289e-01</td>\n",
       "      <td>2.441188e+00</td>\n",
       "      <td>5.808151e-01</td>\n",
       "      <td>1.029241e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>0.056534</td>\n",
       "      <td>0.011886</td>\n",
       "      <td>0.006151</td>\n",
       "      <td>0.008158</td>\n",
       "      <td>0.001351</td>\n",
       "      <td>0.001117</td>\n",
       "      <td>0.012068</td>\n",
       "      <td>0.001219</td>\n",
       "      <td>1.431667e+00</td>\n",
       "      <td>8.775720e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.527563e+06</td>\n",
       "      <td>2.771895e+01</td>\n",
       "      <td>2.451507e+06</td>\n",
       "      <td>1.040778e+05</td>\n",
       "      <td>1.054819e+00</td>\n",
       "      <td>2.458644e-01</td>\n",
       "      <td>4.555570e-01</td>\n",
       "      <td>4.712994e+00</td>\n",
       "      <td>1.854332e+00</td>\n",
       "      <td>1.970386e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>0.215175</td>\n",
       "      <td>0.086906</td>\n",
       "      <td>0.058828</td>\n",
       "      <td>0.042429</td>\n",
       "      <td>0.017461</td>\n",
       "      <td>0.016391</td>\n",
       "      <td>0.089072</td>\n",
       "      <td>0.014114</td>\n",
       "      <td>1.763593e+01</td>\n",
       "      <td>4.331605e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>5.984800e+04</td>\n",
       "      <td>2.000000e+00</td>\n",
       "      <td>6.100600e+04</td>\n",
       "      <td>2.006000e+03</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>3.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>8.565798e+05</td>\n",
       "      <td>2.100000e+01</td>\n",
       "      <td>7.930110e+05</td>\n",
       "      <td>1.600038e+05</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>4.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5.340220e+06</td>\n",
       "      <td>5.400000e+01</td>\n",
       "      <td>5.217531e+06</td>\n",
       "      <td>3.319250e+05</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>4.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>5.955782e+06</td>\n",
       "      <td>5.400000e+01</td>\n",
       "      <td>5.774684e+06</td>\n",
       "      <td>3.662270e+05</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>3.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.666667e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>6.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>7.194640e+06</td>\n",
       "      <td>1.150000e+02</td>\n",
       "      <td>6.333965e+06</td>\n",
       "      <td>3.995440e+05</td>\n",
       "      <td>1.020000e+02</td>\n",
       "      <td>2.100000e+01</td>\n",
       "      <td>3.100000e+01</td>\n",
       "      <td>3.000000e+02</td>\n",
       "      <td>1.870000e+02</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>1.866000e+03</td>\n",
       "      <td>4.936000e+03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 42 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id  publication_id     parent_id    article_id         funny  \\\n",
       "count  1.999516e+06    1.999516e+06  1.134709e+06  1.999516e+06  1.999516e+06   \n",
       "mean   4.065400e+06    4.988997e+01  3.715138e+06  2.810257e+05  2.776687e-01   \n",
       "std    2.527563e+06    2.771895e+01  2.451507e+06  1.040778e+05  1.054819e+00   \n",
       "min    5.984800e+04    2.000000e+00  6.100600e+04  2.006000e+03  0.000000e+00   \n",
       "25%    8.565798e+05    2.100000e+01  7.930110e+05  1.600038e+05  0.000000e+00   \n",
       "50%    5.340220e+06    5.400000e+01  5.217531e+06  3.319250e+05  0.000000e+00   \n",
       "75%    5.955782e+06    5.400000e+01  5.774684e+06  3.662270e+05  0.000000e+00   \n",
       "max    7.194640e+06    1.150000e+02  6.333965e+06  3.995440e+05  1.020000e+02   \n",
       "\n",
       "                wow           sad         likes      disagree      toxicity  \\\n",
       "count  1.999516e+06  1.999516e+06  1.999516e+06  1.999516e+06  1.999516e+06   \n",
       "mean   4.437174e-02  1.089289e-01  2.441188e+00  5.808151e-01  1.029241e-01   \n",
       "std    2.458644e-01  4.555570e-01  4.712994e+00  1.854332e+00  1.970386e-01   \n",
       "min    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "25%    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "50%    0.000000e+00  0.000000e+00  1.000000e+00  0.000000e+00  0.000000e+00   \n",
       "75%    0.000000e+00  0.000000e+00  3.000000e+00  0.000000e+00  1.666667e-01   \n",
       "max    2.100000e+01  3.100000e+01  3.000000e+02  1.870000e+02  1.000000e+00   \n",
       "\n",
       "       ...          white          asian         latino  \\\n",
       "count  ...  448000.000000  448000.000000  448000.000000   \n",
       "mean   ...       0.056534       0.011886       0.006151   \n",
       "std    ...       0.215175       0.086906       0.058828   \n",
       "min    ...       0.000000       0.000000       0.000000   \n",
       "25%    ...       0.000000       0.000000       0.000000   \n",
       "50%    ...       0.000000       0.000000       0.000000   \n",
       "75%    ...       0.000000       0.000000       0.000000   \n",
       "max    ...       1.000000       1.000000       1.000000   \n",
       "\n",
       "       other_race_or_ethnicity  physical_disability  \\\n",
       "count            448000.000000        448000.000000   \n",
       "mean                  0.008158             0.001351   \n",
       "std                   0.042429             0.017461   \n",
       "min                   0.000000             0.000000   \n",
       "25%                   0.000000             0.000000   \n",
       "50%                   0.000000             0.000000   \n",
       "75%                   0.000000             0.000000   \n",
       "max                   1.000000             1.000000   \n",
       "\n",
       "       intellectual_or_learning_disability  psychiatric_or_mental_illness  \\\n",
       "count                        448000.000000                  448000.000000   \n",
       "mean                              0.001117                       0.012068   \n",
       "std                               0.016391                       0.089072   \n",
       "min                               0.000000                       0.000000   \n",
       "25%                               0.000000                       0.000000   \n",
       "50%                               0.000000                       0.000000   \n",
       "75%                               0.000000                       0.000000   \n",
       "max                               1.000000                       1.000000   \n",
       "\n",
       "       other_disability  identity_annotator_count  toxicity_annotator_count  \n",
       "count     448000.000000              1.999516e+06              1.999516e+06  \n",
       "mean           0.001219              1.431667e+00              8.775720e+00  \n",
       "std            0.014114              1.763593e+01              4.331605e+01  \n",
       "min            0.000000              0.000000e+00              3.000000e+00  \n",
       "25%            0.000000              0.000000e+00              4.000000e+00  \n",
       "50%            0.000000              0.000000e+00              4.000000e+00  \n",
       "75%            0.000000              0.000000e+00              6.000000e+00  \n",
       "max            0.600000              1.866000e+03              4.936000e+03  \n",
       "\n",
       "[8 rows x 42 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toxic.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d1647f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    0.941048\n",
       "1.0    0.058952\n",
       "Name: toxicity, dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toxic[\"toxicity\"].round().value_counts() / len(toxic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2231d31",
   "metadata": {},
   "source": [
    "### Summary\n",
    "After our initial look into the data we found a few things to take note of:\n",
    "* We're missing text in a few instances (handle this in preprocessing).\n",
    "* Our target column `toxicity` is heavily skewed towards 0 meaning we'll have to deal with a class imbalance when it comes to modeling (~5% of comments are toxic).\n",
    "* Many of the subtype attributes regarding identities are 77% null meaning we might not be able to make use of these tags during error analysis.  However, the sub-toxic attributes are complete and will be helpful understanding where our model is misclassifying toxic comments. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c6c441",
   "metadata": {},
   "source": [
    "## Test Set\n",
    "Before we dive deep into the dataset we need to create a test set.  Our test set will be 5% (~100,000 instances) which is plenty big enough to provide accurate statistics in our modeling phase. Since we plan on building deep learning models which benefit greatly from large amounts of data this will allow us to allocate 95% of our dataset for training.  \n",
    "\n",
    "Since our dataset is imbalanced we will use stratified sampling to preserve the ratio of classes in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4b3861cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = toxic['comment_text']\n",
    "y = toxic['toxicity'].round()\n",
    "\n",
    "def create_test(X, y, test_size=0.05):\n",
    "    \"\"\"\n",
    "    Create holdout data with stratified sampling techniques.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array-like of shape (n_samples, n_features)\n",
    "        Raw feature set to be split into training and test set.\n",
    "    test_size : float, default=0.05\n",
    "        Decimal value representing percentage of dataset used to create holdout set.\n",
    "        \n",
    "    Notes\n",
    "    -----\n",
    "    This function only creates the test set if it has not been created yet.  \n",
    "    \"\"\"\n",
    "    # Check if test set exists.\n",
    "    TEST_PATH = os.path.join(DATA_DIR, \"interim/test.csv\")\n",
    "    if not os.path.exists(TEST_PATH):\n",
    "        \n",
    "        # Train test split with stratified sampling of the target.\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size,\n",
    "                                                            stratify=y)\n",
    "        \n",
    "        # Combine target and text into train and test sets.\n",
    "        train = pd.concat([X_train, y_train], axis=1)\n",
    "        test = pd.concat([X_test, y_test], axis=1)\n",
    "        \n",
    "        # Save train and test set in directory ~/data/interim\n",
    "        path = os.path.join(DATA_DIR, \"interim\")\n",
    "        train.to_csv(os.path.join(path, \"train.csv\"), index=False)\n",
    "        test.to_csv(os.path.join(path, \"test.csv\"), index=False)\n",
    "\n",
    "create_test(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c034a3",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "70dd74c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "toxic = load_data(path=\"interim/train.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "toxic",
   "language": "python",
   "name": "toxic"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
